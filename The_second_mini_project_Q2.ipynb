{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPOogPYjpkNbDpWMX4rmRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MR-Toufigh/Basics-of-intelligent-systems-fall-2024/blob/main/The_second_mini_project_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress all warnings to keep the output clean\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Upgrade the kagglehub library silently\n",
        "!pip install --upgrade kagglehub -q\n",
        "\n",
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Destination path where the files will be moved\n",
        "destination_path = \"/content\"\n",
        "\n",
        "# Ensure the destination directory exists, create it if necessary\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Download files from the Kaggle dataset\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"aslkuscu/telecust1000t\")\n",
        "    print(f\"Dataset downloaded to temporary path: {path}\")\n",
        "\n",
        "    # Move all files and folders from the temporary path to the destination path\n",
        "    for item in os.listdir(path):\n",
        "        s = os.path.join(path, item)  # Source path of the item\n",
        "        d = os.path.join(destination_path, item)  # Destination path of the item\n",
        "\n",
        "        if os.path.isdir(s):\n",
        "            # If the item is a directory, use copytree to copy it\n",
        "            shutil.copytree(s, d, dirs_exist_ok=True)\n",
        "        else:\n",
        "            # If the item is a single file, use copy2 to copy it\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    print(f\"All files and folders moved to: {destination_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Handle any errors during the process\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "6YOQksC_4g0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/teleCust1000t.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Analyze the dataset\n",
        "print(\"Dataset Information:\")\n",
        "data.info()\n",
        "print(\"\\nDataset Description:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Visualize the dataset\n",
        "# Plot histograms of all features\n",
        "print(\"\\nHistograms of all features:\")\n",
        "data.hist(bins=20, figsize=(15, 10))\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "print(\"\\nCorrelation Heatmap:\")\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Find the two most correlated features with the target column 'custcat'\n",
        "if 'custcat' in data.columns:\n",
        "    target_corr = correlation_matrix['custcat'].sort_values(ascending=False)\n",
        "    most_correlated = target_corr.index[1:3]  # Exclude the target itself\n",
        "    print(\"\\nMost correlated features with 'custcat':\", most_correlated.tolist())\n",
        "else:\n",
        "    print(\"Target column 'custcat' not found in the dataset.\")\n",
        "    most_correlated = None\n",
        "\n",
        "# Visualize histograms of the most correlated features if available\n",
        "if most_correlated is not None:\n",
        "    for feature in most_correlated:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(data[feature], kde=True, bins=20)\n",
        "        plt.title(f\"Distribution of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "cpq21Rmp4hsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/teleCust1000t.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Apply true one-hot encoding directly in the same columns for features with less than 100 unique classes\n",
        "def true_one_hot_encode_inplace(df, columns):\n",
        "    for col in columns:\n",
        "        unique_vals = df[col].unique()\n",
        "        unique_count = len(unique_vals)\n",
        "        value_map = {val: [1 if i == idx else 0 for i in range(unique_count)] for idx, val in enumerate(sorted(unique_vals))}\n",
        "        df[col] = df[col].map(value_map).apply(lambda x: sum([bit * (2 ** idx) for idx, bit in enumerate(x)]))\n",
        "    return df\n",
        "\n",
        "categorical_columns = [col for col in data.columns if data[col].nunique() < 100]\n",
        "data = true_one_hot_encode_inplace(data, categorical_columns)\n",
        "\n",
        "# Analyze the dataset\n",
        "print(\"Dataset Information:\")\n",
        "data.info()\n",
        "print(\"\\nDataset Description:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Visualize the dataset\n",
        "# Plot histograms of all features\n",
        "print(\"\\nHistograms of all features:\")\n",
        "data.hist(bins=20, figsize=(15, 10))\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "print(\"\\nCorrelation Heatmap:\")\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Find the two most correlated features with the target column 'custcat'\n",
        "if 'custcat' in data.columns:\n",
        "    target_corr = correlation_matrix['custcat'].sort_values(ascending=False)\n",
        "    most_correlated = target_corr.index[1:3]  # Exclude the target itself\n",
        "    print(\"\\nMost correlated features with 'custcat':\", most_correlated.tolist())\n",
        "else:\n",
        "    print(\"Target column 'custcat' not found in the dataset.\")\n",
        "    most_correlated = None\n",
        "\n",
        "# Visualize histograms of the most correlated features if available\n",
        "if most_correlated is not None:\n",
        "    for feature in most_correlated:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(data[feature], kde=True, bins=20)\n",
        "        plt.title(f\"Distribution of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "GxLNg5eW5IeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "# Normalize the data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "data[data.columns] = scaler.fit_transform(data[data.columns])\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Output information about the splits\n",
        "print(\"Train Set Size:\", train_data.shape)\n",
        "print(\"Validation Set Size:\", validation_data.shape)\n",
        "print(\"Test Set Size:\", test_data.shape)\n",
        "\n",
        "# Save the splits to separate files if needed\n",
        "train_data.to_csv('/content/train_data.csv', index=False)\n",
        "validation_data.to_csv('/content/validation_data.csv', index=False)\n",
        "test_data.to_csv('/content/test_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "xl06cS374_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF6cOSHNBILV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "# 0) خواندن داده و آماده‌سازی (در صورت نیاز مسیر فایل را اصلاح کنید)\n",
        "#---------------------------------------------------------------------\n",
        "df = pd.read_csv(\"teleCust1000t.csv\")  # مسیر فایل CSV خود را قرار دهید\n",
        "\n",
        "X = df.drop(['custcat'], axis=1).values\n",
        "y = df['custcat'].values\n",
        "y_categorical = to_categorical(y - 1)  # چون مقدار کلاس‌ها از 1 تا 4 است\n",
        "\n",
        "# تقسیم به train/val/test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_categorical, test_size=0.3, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# نرمال‌سازی ویژگی‌ها با MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# فقط برای نمایش شکل داده\n",
        "print(\"Train shape:\", X_train.shape, \" Val shape:\", X_val.shape, \" Test shape:\", X_test.shape)\n",
        "\n",
        "#---------------------------------------------------\n",
        "# توابع کمکی برای ساخت مدل‌های مختلف\n",
        "#---------------------------------------------------\n",
        "\n",
        "def build_model_single_hidden(n_neurons, use_batchnorm=False):\n",
        "    \"\"\"\n",
        "    ساخت یک شبکه‌ی عصبی با تنها یک لایه‌ی مخفی\n",
        "    بهینه‌ساز: SGD\n",
        "    پارامتر use_batchnorm تعیین می‌کند که از BatchNormalization استفاده بشود یا خیر.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # لایه‌ی مخفی\n",
        "    model.add(Dense(n_neurons, input_dim=X_train.shape[1], kernel_initializer='he_uniform'))\n",
        "    if use_batchnorm:\n",
        "        model.add(BatchNormalization())  # لایه‌ی نرمال‌سازی دسته‌ای\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # لایه‌ی خروجی (4 کلاس)\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    # کامپایل با بهینه‌ساز SGD\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_two_hidden(n1, n2, use_batchnorm=False):\n",
        "    \"\"\"\n",
        "    ساخت یک شبکه‌ی عصبی با دو لایه‌ی مخفی\n",
        "    بهینه‌ساز: SGD\n",
        "    پارامترهای n1, n2 برای تعداد نورون‌های لایه‌های مخفی است.\n",
        "    پارامتر use_batchnorm تعیین می‌کند که از BatchNormalization در هر لایه‌ی مخفی استفاده بشود یا خیر.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # لایه‌ی مخفی اول\n",
        "    model.add(Dense(n1, input_dim=X_train.shape[1], kernel_initializer='he_uniform'))\n",
        "    if use_batchnorm:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # لایه‌ی مخفی دوم\n",
        "    model.add(Dense(n2, kernel_initializer='he_uniform'))\n",
        "    if use_batchnorm:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # لایه‌ی خروجی\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    # کامپایل با بهینه‌ساز SGD\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#----------------------------------------------------------\n",
        "# توابع برای نمایش ماتریس سردرگمی و محاسبه دقت هر کلاس\n",
        "#----------------------------------------------------------\n",
        "\n",
        "def display_confusion_matrix_with_precision(y_true, y_pred, class_names):\n",
        "    \"\"\"\n",
        "    Displays a confusion matrix with a precision score for each class\n",
        "    and highlights the confusion matrix in green.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (array-like): True labels\n",
        "        y_pred (array-like): Predicted labels\n",
        "        class_names (list): Names of the classes\n",
        "\n",
        "    \"\"\"\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Compute precision for each class\n",
        "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    # Print precision for each class\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"Precision for {class_name}: {precision_per_class[i]:.2f}\")\n",
        "\n",
        "#----------------------------------------\n",
        "# 1) مدل اول: یک لایه مخفی - حلقه روی n\n",
        "#----------------------------------------\n",
        "\n",
        "neuron_options = [8, 16, 32, 64, 128, 256, 512, 1024]  # به عنوان نمونه\n",
        "additional_numbers = list(range(85, 585, 100))\n",
        "neuron_options.extend(additional_numbers)\n",
        "neuron_options = list(set(neuron_options))  # حذف مقادیر تکراری\n",
        "neuron_options.sort()  # مرتب‌سازی دوباره\n",
        "print(\"\\n======================== MODEL 1 (One Hidden Layer) ========================\")\n",
        "all_model_1_no_bn = []  # To save results for graph\n",
        "all_model_1_bn = []     # To save results for graph\n",
        "\n",
        "for n in neuron_options:\n",
        "    print(f\"\\n>> Single hidden layer with {n} neurons\")\n",
        "    model1 = build_model_single_hidden(n_neurons=n, use_batchnorm=False)\n",
        "\n",
        "    # آموزش مدل:\n",
        "    history1 = model1.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        epochs=100, batch_size=32, verbose=0\n",
        "    )\n",
        "\n",
        "    # دقت پایانی روی داده‌ی train و val\n",
        "    train_acc = history1.history['accuracy'][-1]\n",
        "    val_acc   = history1.history['val_accuracy'][-1]\n",
        "    print(f\"    Final Train Accuracy: {train_acc*100:.2f}%   |   Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # ذخیره‌ی نتایج برای رسم نمودار\n",
        "    all_model_1_no_bn.append((n, train_acc, val_acc))\n",
        "\n",
        "    # پیش‌بینی داده تست و نمایش ماتریس سردرگمی\n",
        "    y_pred = np.argmax(model1.predict(X_test_scaled), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    display_confusion_matrix_with_precision(y_true, y_pred, ['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "\n",
        "#---------------------------------------\n",
        "# 2) مدل دوم: دو لایه مخفی - حلقه تو در تو\n",
        "#---------------------------------------\n",
        "\n",
        "print(\"\\n======================== MODEL 2 (Two Hidden Layers) ========================\")\n",
        "\n",
        "neuron_options_layer1 = [8, 16, 32, 64, 100, 128, 200, 256, 300, 400, 512, 1024]\n",
        "neuron_options_layer2 = [8, 16, 32, 64, 100, 128, 200, 256]\n",
        "\n",
        "all_model_2_no_bn = []  # To save results for graph\n",
        "all_model_2_bn = []     # To save results for graph\n",
        "\n",
        "for n1 in neuron_options_layer1:\n",
        "    for n2 in neuron_options_layer2:\n",
        "        print(f\"\\n>> Two hidden layers with {n1} and {n2} neurons\")\n",
        "        model2 = build_model_two_hidden(n1=n1, n2=n2, use_batchnorm=False)\n",
        "\n",
        "        # آموزش مدل:\n",
        "        history2 = model2.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            validation_data=(X_val_scaled, y_val),\n",
        "            epochs=100, batch_size=32, verbose=0\n",
        "        )\n",
        "\n",
        "        train_acc = history2.history['accuracy'][-1]\n",
        "        val_acc   = history2.history['val_accuracy'][-1]\n",
        "        print(f\"    Final Train Accuracy: {train_acc*100:.2f}%   |   Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # ذخیره‌ی نتایج برای رسم نمودار\n",
        "        all_model_2_no_bn.append(((n1, n2), train_acc, val_acc))\n",
        "\n",
        "        # پیش‌بینی داده تست و نمایش ماتریس سردرگمی\n",
        "        y_pred = np.argmax(model2.predict(X_test_scaled), axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "        display_confusion_matrix_with_precision(y_true, y_pred, ['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# 3) یک حلقه‌ی دیگر برای بررسی اثر لایه‌ی Batch Normalization روی همان مدل‌ها\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n======================== MODEL 1 with BatchNorm (One Hidden Layer) ========================\")\n",
        "for n in neuron_options:\n",
        "    print(f\"\\n>> Single hidden layer with {n} neurons + BatchNorm\")\n",
        "    model1_bn = build_model_single_hidden(n_neurons=n, use_batchnorm=True)\n",
        "\n",
        "    history1_bn = model1_bn.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        epochs=100, batch_size=32, verbose=0\n",
        "    )\n",
        "    train_acc = history1_bn.history['accuracy'][-1]\n",
        "    val_acc   = history1_bn.history['val_accuracy'][-1]\n",
        "    print(f\"    Final Train Accuracy: {train_acc*100:.2f}%   |   Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # ذخیره‌ی نتایج برای رسم نمودار\n",
        "    all_model_1_bn.append((n, train_acc, val_acc))\n",
        "\n",
        "    # پیش‌بینی داده تست و نمایش ماتریس سردرگمی\n",
        "    y_pred = np.argmax(model1_bn.predict(X_test_scaled), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    display_confusion_matrix_with_precision(y_true, y_pred, ['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "\n",
        "print(\"\\n======================== MODEL 2 with BatchNorm (Two Hidden Layers) ========================\")\n",
        "for n1 in neuron_options_layer1:\n",
        "    for n2 in neuron_options_layer2:\n",
        "        print(f\"\\n>> Two hidden layers with {n1} and {n2} neurons + BatchNorm\")\n",
        "        model2_bn = build_model_two_hidden(n1=n1, n2=n2, use_batchnorm=True)\n",
        "\n",
        "        history2_bn = model2_bn.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            validation_data=(X_val_scaled, y_val),\n",
        "            epochs=100, batch_size=32, verbose=0\n",
        "        )\n",
        "        train_acc = history2_bn.history['accuracy'][-1]\n",
        "        val_acc   = history2_bn.history['val_accuracy'][-1]\n",
        "        print(f\"    Final Train Accuracy: {train_acc*100:.2f}%   |   Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # ذخیره‌ی نتایج برای رسم نمودار\n",
        "        all_model_2_bn.append(((n1, n2), train_acc, val_acc))\n",
        "\n",
        "        # پیش‌بینی داده تست و نمایش ماتریس سردرگمی\n",
        "        y_pred = np.argmax(model2_bn.predict(X_test_scaled), axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "        display_confusion_matrix_with_precision(y_true, y_pred, ['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "\n",
        "print(\"\\n==== All experiments finished. ====\")\n",
        "\n",
        "#-----------------------------------------------\n",
        "# Plotting the results for comparison\n",
        "#-----------------------------------------------\n",
        "\n",
        "def plot_results(all_results, title):\n",
        "    neurons = [x[0] for x in all_results]\n",
        "    train_acc = [x[1] for x in all_results]\n",
        "    val_acc = [x[2] for x in all_results]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(neurons, train_acc, label='Train Accuracy')\n",
        "    plt.plot(neurons, val_acc, label='Validation Accuracy')\n",
        "    plt.xlabel('Number of Neurons')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_results(all_model_1_no_bn, 'Model 1 (One Hidden Layer) Without BatchNorm')\n",
        "plot_results(all_model_1_bn, 'Model 1 (One Hidden Layer) With BatchNorm')\n",
        "\n",
        "# Plotting results for Model 2\n",
        "\n",
        "def plot_results_model_2(all_results, title):\n",
        "    neurons_config = [f\"{x[0][0]}-{x[0][1]}\" for x in all_results]\n",
        "    train_acc = [x[1] for x in all_results]\n",
        "    val_acc = [x[2] for x in all_results]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(neurons_config, train_acc, label='Train Accuracy')\n",
        "    plt.plot(neurons_config, val_acc, label='Validation Accuracy')\n",
        "    plt.xlabel('Neurons Configuration (Layer1-Layer2)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "plot_results_model_2(all_model_2_no_bn, 'Model 2 (Two Hidden Layers) Without BatchNorm')\n",
        "plot_results_model_2(all_model_2_bn, 'Model 2 (Two Hidden Layers) With BatchNorm')\n"
      ]
    }
  ]
}