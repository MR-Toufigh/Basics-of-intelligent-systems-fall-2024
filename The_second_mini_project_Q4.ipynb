{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhT48EPwyEEwI+BV+LSR6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MR-Toufigh/Basics-of-intelligent-systems-fall-2024/blob/main/The_second_mini_project_Q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPiLtA0NKtW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cdebf8-e54d-48ce-bb20-61a0a9d2afea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['betas'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and preprocess the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Normalize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Neural Network with RBF layer\n",
        "class RBFLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_units, gamma):\n",
        "        super(RBFLayer, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.centers = self.add_weight(name='centers',\n",
        "                                       shape=(self.num_units, input_shape[-1]),\n",
        "                                       initializer='uniform',\n",
        "                                       trainable=True)\n",
        "        self.betas = self.add_weight(name='betas',\n",
        "                                     shape=(self.num_units,),\n",
        "                                     initializer='ones',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        diff = tf.expand_dims(inputs, axis=1) - self.centers\n",
        "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
        "        return tf.exp(-self.gamma * l2)\n",
        "\n",
        "# Create the RBF model\n",
        "rbf_model = Sequential([\n",
        "    Input(shape=(X.shape[1],)),\n",
        "    RBFLayer(num_units=128, gamma=0.1),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "rbf_model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])\n",
        "history_rbf = rbf_model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "# Evaluate RBF model\n",
        "rbf_predictions = rbf_model.predict(X_test)\n",
        "rbf_loss = mean_squared_error(y_test, rbf_predictions)\n",
        "rbf_r2 = r2_score(y_test, rbf_predictions)\n",
        "\n",
        "print(f\"RBF Model Loss: {rbf_loss}\")\n",
        "print(f\"RBF Model R^2 Score: {rbf_r2}\")\n",
        "\n",
        "# Model 2: Fully Connected Dense Neural Network\n",
        "dense_model = Sequential([\n",
        "    Input(shape=(X.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "dense_model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])\n",
        "history_dense = dense_model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "# Evaluate Dense model\n",
        "dense_predictions = dense_model.predict(X_test)\n",
        "dense_loss = mean_squared_error(y_test, dense_predictions)\n",
        "dense_r2 = r2_score(y_test, dense_predictions)\n",
        "\n",
        "print(f\"Dense Model Loss: {dense_loss}\")\n",
        "print(f\"Dense Model R^2 Score: {dense_r2}\")\n",
        "\n",
        "# Create and display confusion matrix for both models\n",
        "# Note: We convert regression output to categorical by thresholding (e.g., above/below median)\n",
        "median_value = np.median(y_test)\n",
        "rbf_classes = (rbf_predictions.flatten() >= median_value).astype(int)\n",
        "dense_classes = (dense_predictions.flatten() >= median_value).astype(int)\n",
        "y_test_classes = (y_test >= median_value).astype(int)\n",
        "\n",
        "rbf_confusion_matrix = confusion_matrix(y_test_classes, rbf_classes)\n",
        "dense_confusion_matrix = confusion_matrix(y_test_classes, dense_classes)\n",
        "\n",
        "# Plot confusion matrices as heatmaps\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "sns.heatmap(rbf_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title(\"RBF Model Confusion Matrix\")\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"Actual\")\n",
        "\n",
        "sns.heatmap(dense_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "axes[1].set_title(\"Dense Model Confusion Matrix\")\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss for both models\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(history_rbf.history['val_mse'], label='RBF Model Validation MSE', linestyle='--')\n",
        "ax.plot(history_rbf.history['mse'], label='RBF Model Training MSE')\n",
        "ax.plot(history_dense.history['val_mse'], label='Dense Model Validation MSE', linestyle='--')\n",
        "ax.plot(history_dense.history['mse'], label='Dense Model Training MSE')\n",
        "ax.set_title(\"Training and Validation MSE\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Mean Squared Error\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}